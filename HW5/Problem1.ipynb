{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Brennan Duff\n",
        "\n",
        "\n",
        "Assignment 5: Text Generation Using LSTM on Project Gutenberg Training Data"
      ],
      "metadata": {
        "id": "FKq0zCKX_NCr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLy4H65LTq3Q",
        "outputId": "10c1a15e-29e2-45a5-9698-3d4ee441c7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         1000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 128)         117248    \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 10000)       1290000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2407248 (9.18 MB)\n",
            "Trainable params: 2407248 (9.18 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.8611\n",
            "Generated text:\n",
            "to be or not to be delights \n",
            "\n",
            "377/377 [==============================] - 107s 279ms/step - loss: 0.8611\n",
            "Epoch 2/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.2573\n",
            "Generated text:\n",
            "to be or not to be yesty thy confess longer . \n",
            "\n",
            "377/377 [==============================] - 105s 279ms/step - loss: 0.2573\n",
            "Epoch 3/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.2401\n",
            "Generated text:\n",
            "to be or not to be and , what . \n",
            "\n",
            "377/377 [==============================] - 106s 280ms/step - loss: 0.2401\n",
            "Epoch 4/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.2297\n",
            "Generated text:\n",
            "to be or not to be for for what them ; he o’ , _ with beseeming \n",
            "\n",
            "377/377 [==============================] - 106s 281ms/step - loss: 0.2297\n",
            "Epoch 5/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.2200\n",
            "Generated text:\n",
            "to be or not to be barnardo , \n",
            "\n",
            "377/377 [==============================] - 106s 280ms/step - loss: 0.2200\n",
            "Epoch 6/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.2119\n",
            "Generated text:\n",
            "to be or not to be general include ? \n",
            "\n",
            "377/377 [==============================] - 105s 278ms/step - loss: 0.2119\n",
            "Epoch 7/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.2054\n",
            "Generated text:\n",
            "to be or not to be looks vpon tell me the drift ' dust . \n",
            "\n",
            "377/377 [==============================] - 105s 279ms/step - loss: 0.2054\n",
            "Epoch 8/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1997\n",
            "Generated text:\n",
            "to be or not to be flaming girdle , lies is the foundation , \n",
            "\n",
            "377/377 [==============================] - 104s 276ms/step - loss: 0.1997\n",
            "Epoch 9/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1944\n",
            "Generated text:\n",
            "to be or not to be dreames vnder away . \n",
            "\n",
            "377/377 [==============================] - 104s 276ms/step - loss: 0.1944\n",
            "Epoch 10/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1895\n",
            "Generated text:\n",
            "to be or not to be circumstance . \n",
            "\n",
            "377/377 [==============================] - 104s 276ms/step - loss: 0.1895\n",
            "Epoch 11/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1847\n",
            "Generated text:\n",
            "to be or not to be pass . \n",
            "\n",
            "377/377 [==============================] - 104s 275ms/step - loss: 0.1847\n",
            "Epoch 12/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1803\n",
            "Generated text:\n",
            "to be or not to be mine accesse \n",
            "\n",
            "377/377 [==============================] - 104s 275ms/step - loss: 0.1803\n",
            "Epoch 13/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1759\n",
            "Generated text:\n",
            "to be or not to be perfect adam’s full archive terms of \n",
            "\n",
            "377/377 [==============================] - 105s 278ms/step - loss: 0.1759\n",
            "Epoch 14/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1717\n",
            "Generated text:\n",
            "to be or not to be our daies ; \n",
            "\n",
            "377/377 [==============================] - 104s 275ms/step - loss: 0.1717\n",
            "Epoch 15/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1677\n",
            "Generated text:\n",
            "to be or not to be coronation , and play , \n",
            "\n",
            "377/377 [==============================] - 104s 277ms/step - loss: 0.1677\n",
            "Epoch 16/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1636\n",
            "Generated text:\n",
            "to be or not to be sunder and done , stand this grosse statists \n",
            "\n",
            "377/377 [==============================] - 105s 278ms/step - loss: 0.1636\n",
            "Epoch 17/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1594\n",
            "Generated text:\n",
            "to be or not to be lowring vault , \n",
            "\n",
            "377/377 [==============================] - 104s 276ms/step - loss: 0.1594\n",
            "Epoch 18/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1554\n",
            "Generated text:\n",
            "to be or not to be thine ; to fly his pluck : \n",
            "\n",
            "377/377 [==============================] - 104s 276ms/step - loss: 0.1554\n",
            "Epoch 19/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1514\n",
            "Generated text:\n",
            "to be or not to be closely , if you nurse : \n",
            "\n",
            "377/377 [==============================] - 104s 277ms/step - loss: 0.1514\n",
            "Epoch 20/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1472\n",
            "Generated text:\n",
            "to be or not to be fill . cleave to go : dry , if my liege , \n",
            "\n",
            "377/377 [==============================] - 105s 278ms/step - loss: 0.1472\n",
            "Epoch 21/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1431\n",
            "Generated text:\n",
            "to be or not to be well , there’s you shall cool’d \n",
            "\n",
            "377/377 [==============================] - 105s 277ms/step - loss: 0.1431\n",
            "Epoch 22/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1390\n",
            "Generated text:\n",
            "to be or not to be that , i am to question . \n",
            "\n",
            "377/377 [==============================] - 105s 278ms/step - loss: 0.1390\n",
            "Epoch 23/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1351\n",
            "Generated text:\n",
            "to be or not to be distracted ; \n",
            "\n",
            "377/377 [==============================] - 104s 276ms/step - loss: 0.1351\n",
            "Epoch 24/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1311\n",
            "Generated text:\n",
            "to be or not to be a nature are maggots . \n",
            "\n",
            "377/377 [==============================] - 105s 277ms/step - loss: 0.1311\n",
            "Epoch 25/25\n",
            "377/377 [==============================] - ETA: 0s - loss: 0.1272\n",
            "Generated text:\n",
            "to be or not to be perfect . \n",
            "\n",
            "377/377 [==============================] - 105s 277ms/step - loss: 0.1272\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef5b85bacb0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses\n",
        "import requests\n",
        "\n",
        "# Constants\n",
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "N_UNITS = 128\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 25\n",
        "\n",
        "# List of URLs for additional texts (Shakespeare plays)\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/files/1524/1524-0.txt\",  # Hamlet\n",
        "    \"https://www.gutenberg.org/files/1533/1533-0.txt\",  # Macbeth\n",
        "    \"https://www.gutenberg.org/files/1112/1112-0.txt\"   # Othello\n",
        "]\n",
        "\n",
        "# Preprocessing function to remove Gutenberg headers and footers\n",
        "def clean_text(text):\n",
        "    start_idx = text.find(\"*** START OF THIS PROJECT GUTENBERG EBOOK\")\n",
        "    end_idx = text.find(\"*** END OF THIS PROJECT GUTENBERG EBOOK\")\n",
        "    if start_idx != -1 and end_idx != -1:\n",
        "        text = text[start_idx:end_idx]\n",
        "    return text.replace(\"\\r\", \"\").strip()\n",
        "\n",
        "# Download and preprocess texts\n",
        "all_text = \"\"\n",
        "for url in urls:\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "    all_text += clean_text(text) + \"\\n\\n\"\n",
        "\n",
        "# Save combined text for confirmation\n",
        "file_path = \"combined_shakespeare.txt\"\n",
        "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(all_text)\n",
        "\n",
        "# Tokenize the entire text into sentences\n",
        "sentences = all_text.split(\"\\n\")\n",
        "sentences = [s for s in sentences if len(s) > 0]  # Filter out empty lines\n",
        "\n",
        "# Pad punctuation\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    return s\n",
        "\n",
        "text_data = [pad_punctuation(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Convert to TensorFlow Dataset\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(text_data).batch(BATCH_SIZE).shuffle(1000, seed=SEED)\n",
        "\n",
        "# Create a vectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Tokenize data for training\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)\n",
        "\n",
        "# Model Definition\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(N_UNITS, return_sequences=True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()\n",
        "\n",
        "# Compile the model\n",
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(optimizer=\"adam\", loss=loss_fn)\n",
        "\n",
        "# Callback for generating text during training\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y = self.model.predict(x, verbose=0)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\nGenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.generate(\"to be or not to be\", max_tokens=100, temperature=1.0)\n",
        "\n",
        "# Prepare callback\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "text_generator = TextGenerator(vocab)\n",
        "\n",
        "# Train the model\n",
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "3Hwim9xninmc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"To be or not to be:\", max_tokens=50, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxPNqGiRgT_z",
        "outputId": "c1f940b2-3307-4b20-c966-4c43bd8b9242"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "To be or not to be: ; for it is a man . \n",
            "\n",
            "\n",
            "PROMPT: To be or not to be:\n",
            ".:   \t92.88%\n",
            ";:   \t6.4%\n",
            ",:   \t0.72%\n",
            "of:   \t0.0%\n",
            "::   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ;\n",
            "for:   \t86.23%\n",
            "i:   \t11.22%\n",
            "and:   \t0.67%\n",
            "so:   \t0.61%\n",
            "the:   \t0.51%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ; for\n",
            "the:   \t42.58%\n",
            "it:   \t17.01%\n",
            "my:   \t13.54%\n",
            "you:   \t10.95%\n",
            "your:   \t9.53%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ; for it\n",
            "is:   \t98.44%\n",
            "not:   \t1.48%\n",
            "be:   \t0.05%\n",
            "i:   \t0.02%\n",
            "will:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ; for it is\n",
            "a:   \t90.64%\n",
            "not:   \t8.37%\n",
            "the:   \t0.82%\n",
            "too:   \t0.11%\n",
            "no:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ; for it is a\n",
            "man:   \t72.46%\n",
            "king:   \t18.56%\n",
            "tale:   \t4.57%\n",
            "little:   \t1.31%\n",
            "day:   \t0.61%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ; for it is a man\n",
            ".:   \t98.93%\n",
            ",:   \t1.07%\n",
            ";:   \t0.0%\n",
            "::   \t0.0%\n",
            ":   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: To be or not to be: ; for it is a man .\n",
            ":   \t100.0%\n",
            "i:   \t0.0%\n",
            "—:   \t0.0%\n",
            "but:   \t0.0%\n",
            "you:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"A rose by any other name would \", max_tokens=100, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-731l5ugwOm",
        "outputId": "230f3f18-6768-4078-cc05-b4188990d288"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "A rose by any other name would  set the work \n",
            "\n",
            "\n",
            "PROMPT: A rose by any other name would \n",
            "set:   \t77.61%\n",
            ":   \t12.36%\n",
            "any:   \t5.06%\n",
            "the:   \t4.59%\n",
            "online:   \t0.17%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: A rose by any other name would  set\n",
            "the:   \t46.84%\n",
            ",:   \t46.46%\n",
            "forth:   \t4.94%\n",
            "a:   \t1.29%\n",
            ":   \t0.33%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: A rose by any other name would  set the\n",
            ":   \t48.57%\n",
            "work:   \t31.27%\n",
            "project:   \t19.99%\n",
            "day:   \t0.16%\n",
            "capulets:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: A rose by any other name would  set the work\n",
            ":   \t99.06%\n",
            ",:   \t0.94%\n",
            ";:   \t0.0%\n",
            "::   \t0.0%\n",
            "!:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Brevity is the\", max_tokens=100, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlXKGk6xhADh",
        "outputId": "7e1bfbdd-ad8b-4a2e-9be7-98e70150bdf3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            "Brevity is the very coinage of my brain . \n",
            "\n",
            "\n",
            "PROMPT: Brevity is the\n",
            "very:   \t72.93%\n",
            "king:   \t26.48%\n",
            "great:   \t0.16%\n",
            "most:   \t0.13%\n",
            "more:   \t0.1%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Brevity is the very\n",
            "witching:   \t35.29%\n",
            "flame:   \t17.38%\n",
            "coinage:   \t16.45%\n",
            "king:   \t13.08%\n",
            "ecstasy:   \t5.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Brevity is the very coinage\n",
            "of:   \t100.0%\n",
            "-:   \t0.0%\n",
            ",:   \t0.0%\n",
            "o’:   \t0.0%\n",
            ".:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Brevity is the very coinage of\n",
            "my:   \t99.95%\n",
            "the:   \t0.03%\n",
            "his:   \t0.01%\n",
            "a:   \t0.01%\n",
            "your:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Brevity is the very coinage of my\n",
            "brain:   \t87.56%\n",
            "life:   \t5.82%\n",
            "blood:   \t2.72%\n",
            "heart:   \t2.23%\n",
            "thoughts:   \t0.54%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Brevity is the very coinage of my brain\n",
            ".:   \t99.68%\n",
            ",:   \t0.31%\n",
            ";:   \t0.01%\n",
            "?:   \t0.0%\n",
            "!:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: Brevity is the very coinage of my brain .\n",
            ":   \t100.0%\n",
            "you:   \t0.0%\n",
            "i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "—:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using 128 units per layer worked the best in terms time-to-train and learning capabilty for this case. Others resulted in either incoherence or disconnected runtimes.\n",
        "\n",
        "The temperature set at .2 worked the best for coherence, the other temperatures I tested were either incoherent, or inferior to this temperature. \"Brevity is the very coinage of my brain\" somewhat captures the style of Shakespeare (being generous), so I thought this temperature worked the best.\n",
        "\n",
        "The generated text overall was decent at .2 and lackluster at other temperatures. Other temperatures were not coherent."
      ],
      "metadata": {
        "id": "GYaPWJR5KufX"
      }
    }
  ]
}