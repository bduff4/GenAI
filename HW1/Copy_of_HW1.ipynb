{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* Brennan Duff\n",
        "* Generative AI D01\n",
        "* 1/28/2026\n",
        "* Assignment 1, The objective is to observe and report on how the temperature parameter alters the confidence of an LLM and impacts the logical coherence of its output."
      ],
      "metadata": {
        "id": "9kaHYrUsT8Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install & import the needed libraries\n",
        "\n",
        "!pip install -q transformers torch # tensor operations & model execution\n",
        "\n",
        "!pip install triton torchao # performance libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "OCZtX6byUBu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import os\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\" # Disables progress bar widgets error caused by GPT\n"
      ],
      "metadata": {
        "id": "VH5GmrTgUI8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer & model\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # converts text into token IDs\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # language model\n",
        "model.eval() # prevent dropout\n"
      ],
      "metadata": {
        "id": "M2RFvfcyUbX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your own input text\n",
        "\n",
        "text = input(\"Enter a sentence: \") # accepts user input\n"
      ],
      "metadata": {
        "id": "meoOLg5yUmQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The tokenization step typically creates subword tokens, and not necessarily whole words\n",
        "\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "\n",
        "print(\"Token IDs:\", tokens.tolist()[0]) # display token ID and string representation\n",
        "print(\"Tokens:\")\n",
        "for tid in tokens[0]:\n",
        "    print(f\"{tid.item():>6} → '{tokenizer.decode(tid)}'\")\n"
      ],
      "metadata": {
        "id": "HNzXV2M7U6CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The embeddings\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Token embeddings\n",
        "    token_embeds = model.transformer.wte(tokens)\n",
        "\n",
        "    # Positional embeddings\n",
        "    positions = torch.arange(tokens.size(1)).unsqueeze(0)\n",
        "    pos_embeds = model.transformer.wpe(positions)\n",
        "\n",
        "    # final input embeddings passed into transformer\n",
        "    embeddings = token_embeds + pos_embeds\n",
        "\n",
        "# (batch_size, sequence_length, embedding_dim)\n",
        "print(\"Embedding shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "id": "hWWDeF8uVT9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The transformer forward pass ensures that each token now contains contextual information from previous tokens.\n",
        "# This is the most important step conceptually, because this is where the model goes from isolated words to understanding a sentence.\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # Send the embedding vectors through all transformer layers (for GPT-2, it is 12 layers)\n",
        "    outputs = model.transformer(inputs_embeds=embeddings)\n",
        "\n",
        "    # Each layer, applies the self-attention mechanism and goes through a feed-forward NN\n",
        "    hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"Hidden state shape:\", hidden_states.shape)\n"
      ],
      "metadata": {
        "id": "MTYhtAE8VqEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logits for the next token. This gives one score per vocabulary token (~50k tokens)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden = hidden_states[:, -1, :]\n",
        "    logits = model.lm_head(last_hidden)\n",
        "\n",
        "print(\"Logits shape:\", logits.shape)\n"
      ],
      "metadata": {
        "id": "IxcEuLd1V4Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax → probabilities: this is the actual probability distribution the model uses\n",
        "\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "top_probs, top_ids = torch.topk(probs, k=10) # top 10 most likely next tokens\n",
        "\n",
        "print(\"Top 10 next-token probabilities:\")\n",
        "for p, tid in zip(top_probs[0], top_ids[0]):\n",
        "    token = tokenizer.decode(tid)\n",
        "    print(f\"{token!r:>12} : {p.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "OvRMw5ROWGvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling (temperature + top-k)\n",
        "\n",
        "#    temperature = 0.2 (Set a low temperature value to generate predictable responses)\n",
        "#    temperature = 1.5 (Set a high temperature value to generate more random and creative responses)\n",
        "#    top_k = None (full distribution)\n",
        "\n",
        "def sample_next_token(logits, temperature=2.0, top_k=40):\n",
        "    logits = logits / temperature\n",
        "\n",
        "    if top_k is not None:\n",
        "        values, indices = torch.topk(logits, top_k) # keep only top_k logits\n",
        "        probs = F.softmax(values, dim=-1)\n",
        "        choice = torch.multinomial(probs, 1) # sample from restricted distribution\n",
        "        return indices[0, choice]\n",
        "    else:\n",
        "        probs = F.softmax(logits, dim=-1) # sample from full vocabulary\n",
        "        return torch.multinomial(probs, 1)\n",
        "\n",
        "next_token_id = sample_next_token(logits, temperature=2.0, top_k=40) # sample single next token\n",
        "print(\"Sampled token:\", tokenizer.decode(next_token_id[0]))\n"
      ],
      "metadata": {
        "id": "XXilv6iIWS4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full loop (generate multiple tokens)\n",
        "\n",
        "def generate_step_by_step(prompt, steps=20):\n",
        "    tokens = tokenizer.encode(prompt, return_tensors=\"pt\") # encode initial prompt\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(tokens) # full model forward pass\n",
        "            logits = outputs.logits[:, -1, :] # logit for the last token\n",
        "            next_token = sample_next_token(logits, temperature=2.0, top_k=40) # sample next token\n",
        "\n",
        "        tokens = torch.cat([tokens, next_token], dim=1) # append token to sequence\n",
        "        print(tokenizer.decode(tokens[0])) # print decoded output\n",
        "\n",
        "generate_step_by_step(text, steps=20) # generate starting from user input\n"
      ],
      "metadata": {
        "id": "7Y-vgMelWkKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiments:\n",
        "\n",
        "\n",
        "| Trial |\tTemperature (T) |\tPredicted Behavior | Model Response | Model Coherence (1-10) |\n",
        "|-------|-----------------|--------------------|----------------|-----------------|\n",
        "\"The dog ran around the park\" |\t0.1\t| Conservative | \"The dog ran around the park, trying to get away from its owners.(\\n)An officer arrived on scene and shot the dog\"| 10 |\n",
        "\"A dog ran down the sidewalk\" |\t0.8\t| Creative | \"A dog ran down the sidewalk and struck a child in the leg and a man in the head.(\\n)Authorities said the man\"| 9 |\n",
        "\"The dog ran across the field\" |\t2.0\t| Chaos | \"The dog ran across the field without warning as they looked for help and stopped him, leaving her trapped underneath two big crates and in\"| 6 |"
      ],
      "metadata": {
        "id": "oevVI7oxKzta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis:\n",
        "\n",
        "* *Did your model repeat any words or phrases?*\n",
        "  * Aside from common words (the, a, and, etc.), the model did not repeat any specific word or phrase more than others.\n",
        "\n",
        "* *Did the model use real words, or did it start outputting random characters and punctuation? Explain how the \"Probability Distribution\" changed to allow this.*\n",
        "  * The model did not output random characters or punctuation over the course of my testing. However, if it were to happen, it would have to be because a higher temperature setting, such as 2.0, allows for more low-probability tokens to be selected, as it would flatten the probability distribution.\n",
        "\n",
        "* *If you were building a medical AI to give prescriptions or advice, which temperature would you use?*\n",
        "  * I would use a temperature of .1, as it is the most logical and coherent of the 3 tested values.\n",
        "\n",
        "* *If you were building an AI to write a surrealist dream-journal, which would you use?*\n",
        "  * I would use a temperature of 2.0, as the higher temperature allows for more surreal and abstract scenarios."
      ],
      "metadata": {
        "id": "JZBofMjIYfe-"
      }
    }
  ]
}